# Neural Machine Translation - Coverage and Attention

## Models

Using implementations of following paper make 4 translation models and check their performance on test data

- [Sequence to Sequence Learning with Neural Networks](papers/seq2seq.pdf)
- [Neural Machine Translation By Jointly Learning To Align And Translate](papers/enc-dec.pdf) Basic attention
- [Effective Approaches to Attention-based Neural Machine Translation](papers/attention.pdf) Try the three global attention options, ignore local attention
- [Modeling Coverage for Neural Machine Translation](papers/coverage.pdf) Does this help for repeated words ?

## Dataset

[Enclosed](https://drive.google.com/open?id=1ilP2_6N0clzTm8IWCQ0MC7MxrJXuOiH3) are parallel corpus
- English Bangla Gujurati Hindi Konkani Malayalam Marathi Punjabi Tamil Telugu
- Kindly choose a pair (En-IL; where IL is one of (Bangla Gujurati Hindi Konkani Malayalam Marathi Punjabi Tamil Telugu)).
- Keep in mind while choosing a pair that you have to also evaluate the performance by going through test data.
- Each pair has 3 sets of data- train, dev and test.

## Output

- Keep your code and model (since model's size will be big) on drive and share the link.
- Test input and output (can also be shared via drive)
- Analyse of 50 interesting sentences. 
